import os
import torch
import argparse
import gradio as gr
from zipfile import ZipFile
import langid
import se_extractor
from api import BaseSpeakerTTS, ToneColorConverter

parser = argparse.ArgumentParser()
parser.add_argument("--share", action='store_true', default=False, help="make link public")
args = parser.parse_args()

en_ckpt_base = 'checkpoints/base_speakers/EN'
zh_ckpt_base = 'checkpoints/base_speakers/ZH'
ckpt_converter = 'checkpoints/converter'
device = 'cuda' if torch.cuda.is_available() else 'cpu'
output_dir = 'outputs'
os.makedirs(output_dir, exist_ok=True)

# Load models
en_base_speaker_tts = BaseSpeakerTTS(f'{en_ckpt_base}/config.json', device=device)
en_base_speaker_tts.load_ckpt(f'{en_ckpt_base}/checkpoint.pth')
zh_base_speaker_tts = BaseSpeakerTTS(f'{zh_ckpt_base}/config.json', device=device)
zh_base_speaker_tts.load_ckpt(f'{zh_ckpt_base}/checkpoint.pth')
tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)
tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')

# Load speaker embeddings
en_source_default_se = torch.load(f'{en_ckpt_base}/en_default_se.pth').to(device)
en_source_style_se = torch.load(f'{en_ckpt_base}/en_style_se.pth').to(device)
zh_source_se = torch.load(f'{zh_ckpt_base}/zh_default_se.pth').to(device)

supported_languages = ['zh', 'en']

def predict(prompt, style, audio_file_pth, mic_file_path, use_mic, agree):
    text_hint = ''
    if agree == False:
        text_hint += '[ERROR] Please accept the Terms & Condition!\n'
        return text_hint, None, None

    language_predicted = langid.classify(prompt)[0].strip()  
    if language_predicted not in supported_languages:
        text_hint += f"[ERROR] The detected language {language_predicted} is not supported.\n"
        return text_hint, None, None
    
    if language_predicted == "zh":
        tts_model = zh_base_speaker_tts
        source_se = zh_source_se
        language = 'Chinese'
        if style != 'default':
            text_hint += f"[ERROR] Only 'default' style is supported for Chinese.\n"
            return text_hint, None, None
    else:
        tts_model = en_base_speaker_tts
        source_se = en_source_default_se if style == 'default' else en_source_style_se
        language = 'English'
        if style not in ['default', 'whispering', 'shouting', 'excited', 'cheerful', 'terrified', 'angry', 'sad', 'friendly']:
            text_hint += f"[ERROR] The style {style} is not supported for English.\n"
            return text_hint, None, None

    speaker_wav = mic_file_path if use_mic else audio_file_pth
    if not speaker_wav:
        text_hint += "[ERROR] Please provide or record a reference audio.\n"
        return text_hint, None, None

    if len(prompt) < 2:
        text_hint += "[ERROR] Please provide a longer prompt.\n"
        return text_hint, None, None
    if len(prompt) > 200:
        text_hint += "[ERROR] Text is too long; limit to 200 characters.\n"
        return text_hint, None, None
    
    try:
        target_se, audio_name = se_extractor.get_se(speaker_wav, tone_color_converter, target_dir='processed', vad=True)
    except Exception as e:
        text_hint += f"[ERROR] Tone color extraction error: {str(e)}\n"
        return text_hint, None, None

    src_path = f'{output_dir}/tmp.wav'
    tts_model.tts(prompt, src_path, speaker=style, language=language)
    save_path = f'{output_dir}/output.wav'

    tone_color_converter.convert(
        audio_src_path=src_path, 
        src_se=source_se, 
        tgt_se=target_se, 
        output_path=save_path,
        message="@MyShell"
    )

    text_hint += "Response generated successfully.\n"
    return text_hint, save_path, speaker_wav

title = "MyShell OpenVoice"
description = """
<div style="text-align: center;">
    We introduce OpenVoice, a versatile instant voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. OpenVoice also achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set.
</div>
"""

examples = [
    [
        "今天天气真好，我们一起出去吃饭吧。",
        'default',
        "resources/demo_speaker0.mp3",
        None,
        False,
        True,
    ],
    [
        "This audio is generated by open voice with a half-performance model.",
        'whispering',
        "resources/demo_speaker1.mp3",
        None,
        False,
        True,
    ],
    [
        "He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered, flour-fattened sauce.",
        'sad',
        "resources/demo_speaker2.mp3",
        None,
        False,
        True,
    ],
]

with gr.Blocks(analytics_enabled=False) as demo:
    gr.Markdown(description)
    
    with gr.Row():
        with gr.Column():
            input_text_gr = gr.Textbox(
                label="Text Prompt",
                info="One or two sentences at a time is better. Up to 200 text characters.",
                value="He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered, flour-fattened sauce.",
            )
            style_gr = gr.Dropdown(
                label="Style",
                info="Select a style for the synthesized speech. (Chinese only supports 'default')",
                choices=['default', 'whispering', 'cheerful', 'terrified', 'angry', 'sad', 'friendly'],
                value="default",
            )
            ref_gr = gr.Audio(
                label="Reference Audio",
                info="Click the ✎ button to upload a target speaker audio",
                type="filepath",
                value="resources/demo_speaker0.mp3",
            )
            mic_gr = gr.Audio(
                source="microphone",
                type="filepath",
                info="Use your microphone to record audio",
                label="Use Microphone for Reference",
            )
            use_mic_gr = gr.Checkbox(
                label="Use Microphone",
                value=False,
                info="Notice: Microphone input may not work properly under traffic",
            )
            tos_gr = gr.Checkbox(
                label="Agree",
                value=False,
                info="I agree to the terms of the cc-by-nc-4.0 license",
            )
            tts_button = gr.Button("Send")

        with gr.Column():
            out_text_gr = gr.Text(label="Info")
            audio_gr = gr.Audio(label="Synthesized Audio", autoplay=True)
            ref_audio_gr = gr.Audio(label="Reference Audio Used")

            gr.Examples(
                examples,
                label="Examples",
                inputs=[input_text_gr, style_gr, ref_gr, mic_gr, use_mic_gr, tos_gr],
                outputs=[out_text_gr, audio_gr, ref_audio_gr],
                fn=predict,
                cache_examples=False,
            )

            tts_button.click(
                predict, 
                [input_text_gr, style_gr, ref_gr, mic_gr, use_mic_gr, tos_gr], 
                outputs=[out_text_gr, audio_gr, ref_audio_gr]
            )

demo.queue()
demo.launch(debug=True, show_api=True, share=args.share)
